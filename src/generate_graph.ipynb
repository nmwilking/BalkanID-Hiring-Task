{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "import networkx as nx\n",
    "import json\n",
    "import arxiv\n",
    "from itertools import islice\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "papers = None\n",
    "with open(Config.REDUCED_JSON_PATH, 'r') as infile:\n",
    "    papers = json.load(infile)\n",
    "\n",
    "unarxiv = None\n",
    "with open(Config.UNARXIV_REDUCED_JSON_PATH, 'r') as infile:\n",
    "    unarxiv = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset contains 795 papers making 10918 citations of 8947 unique works, 245 of which reference other papers in the dataset.\n",
      "31 papers in the dataset have both an incoming and outgoing citation in the set.\n"
     ]
    }
   ],
   "source": [
    "# Cursory density estimation\n",
    "\n",
    "l = set(papers.keys())\n",
    "found = {True: 0, False: 0}\n",
    "citing = set()\n",
    "cited = set()\n",
    "all_citations = set()\n",
    "for id, md in papers.items():\n",
    "    for ref in md['arxiv_bib_ids']:\n",
    "        all_citations.add(ref)\n",
    "        if ref in l:\n",
    "            found[True] += 1\n",
    "            citing.add(id)\n",
    "            cited.add(ref)\n",
    "        else:\n",
    "            found[False] += 1\n",
    "\n",
    "print(f'''This dataset contains {len(papers)} papers making {sum(found.values())} citations \\\n",
    "of {len(all_citations)} unique works, {found[True]} of which reference other papers in the dataset.\n",
    "{len(cited.intersection(citing))} papers in the dataset have both an incoming and outgoing citation in the set.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True: 667, False: 10251}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Augmented density estimation\n",
    "\n",
    "found = {True: 0, False: 0}\n",
    "for id, md in papers.items():\n",
    "    for ref in md['arxiv_bib_ids']:\n",
    "        found[ref in unarxiv] += 1\n",
    "found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph()\n",
    "for id, data in unarxiv.items():\n",
    "    for bib_id in data['arxiv_bib_ids']:\n",
    "        # Protection against self-loops and external citations\n",
    "        if id != bib_id and bib_id in unarxiv:\n",
    "            g.add_edge(bib_id, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_directed_acyclic_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch publication dates for relevant papers\n",
    "\n",
    "def batched(iterable, n):\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "client = arxiv.Client()\n",
    "id_strip_re = re.compile(r'(?P<id>.+)v\\d+$')\n",
    "for id_batch in batched(g.nodes, 500):\n",
    "    search = arxiv.Search(\n",
    "        id_list = list(id_batch)\n",
    "    )\n",
    "\n",
    "    for result in client.results(search):\n",
    "        id = result.get_short_id()\n",
    "        m = id_strip_re.match(id)\n",
    "        if m:\n",
    "            id = m.group('id')\n",
    "        if id in unarxiv:\n",
    "            unarxiv[id]['published'] = result.published\n",
    "        else:\n",
    "            print(f'{id} not found')\n",
    "            exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove edges that run counter to chronological order\n",
    "\n",
    "for src_id, dest_id in list(g.edges):\n",
    "    if unarxiv[src_id]['published'] > unarxiv[dest_id]['published']:\n",
    "        g.remove_edge(src_id, dest_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_directed_acyclic_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New density estimation\n",
    "\n",
    "found = {\n",
    "    True: 0,\n",
    "    False: 0\n",
    "}\n",
    "for id in papers:\n",
    "    found[id in g.nodes] += 1\n",
    "\n",
    "citing = set()\n",
    "cited = set()\n",
    "for cited_id, citing_id in g.edges:\n",
    "    citing.add(citing_id)\n",
    "    cited.add(cited_id)\n",
    "\n",
    "\n",
    "print(f'{found[True]} of {len(papers)} papers cite or are cited by another paper in the expanded dataset. {len(citing.intersection(cited).intersection(papers))} of the papers of interest have both an incoming and outgoing citation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write binary dump\n",
    "\n",
    "with open(Config.GRAPH_BIN_PATH, 'wb') as outfile:\n",
    "    pickle.dump(g, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
